<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://dimitri-voytan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dimitri-voytan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T00:32:04+00:00</updated><id>https://dimitri-voytan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Are Fourier Neural Operators Really Faster?</title><link href="https://dimitri-voytan.github.io/blog/2025/fno-seismic/" rel="alternate" type="text/html" title="Are Fourier Neural Operators Really Faster?"/><published>2025-05-26T00:00:00+00:00</published><updated>2025-05-26T00:00:00+00:00</updated><id>https://dimitri-voytan.github.io/blog/2025/fno-seismic</id><content type="html" xml:base="https://dimitri-voytan.github.io/blog/2025/fno-seismic/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Fourier Neural Operators (FNOs) are often advertised as fast, mesh-invariant alternatives to traditional PDE solvers, with some studies claiming massive speedups over finite-difference (FD) methods. But do those claims hold up when we put FNOs head-to-head with optimized, GPU-accelerated FD codes?</p> <p>In this post, we benchmark a state-of-the-art Tucker-tensorized FNO against hand-tuned FD solvers for four seismic wave physics formulations: isotropic acoustic, TTI-acoustic, isotropic elastic, and VTI-elastic. What we find is surprising: FNOs are usually slower—often <em>much</em> slower—unless you’re doing a very specific type of reduced-order modeling.</p> <image></image> <h2 id="background-solving-wave-equations">Background: Solving Wave Equations</h2> <p>Seismic wave simulations solve PDEs that model how waves travel through the Earth. One classic example is the 3D acoustic wave equation:</p> <p>$\frac{\partial^2 u}{\partial t^2} = c^2 \nabla^2 u$</p> <p>FD methods discretize space and time into a grid, then update the wavefield with local stencils. They’re fast, parallelizable, and well-understood. But they get exponentially more expensive as you increase frequency, lower velocity, or demand high accuracy—you need finer grids and smaller time steps due to the Courant-Friedrichs-Lewy (CFL) condition and numerical dispersion.</p> <p>On the other hand, FNOs aim to learn a mapping from input fields (like velocity) to output fields (like pressure or displacement) using deep neural networks. Crucially, FNOs work in the Fourier domain and apply learned convolutions via FFTs.</p> <image></image> <h2 id="experiments">Experiments</h2> <p>We ran both FNOs and FD solvers on the same hardware (NVIDIA A100s) and carefully controlled for fair benchmarking:</p> <ul> <li>Same simulation domain size</li> <li>Same time duration (1/2 second)</li> <li>Same frequency content (30 Hz max)</li> <li>FD grid spacing = 10m (limited by points per wavelength)</li> <li>FNO spacing = 10m and 25m (Nyquist frequency)</li> <li>Varied model complexity: from simple acoustic to full anisotropic elasticity</li> </ul> <p>We didn’t train any models. Instead, we focused only on <em>inference time</em> — the cost of running the forward pass once the model is trained.</p> <h2 id="key-results">Key Results</h2> <h3 id="1-on-identical-grids-10m-spacing-fnos-are-always-slower">1. On identical grids (10m spacing), FNOs are <em>always</em> slower.</h3> <table> <thead> <tr> <th>Physics</th> <th>Small FNO (4 Layers, 12 Channels)</th> <th>Medium FNO (8 Layers, 24 Channels)</th> <th>Large FNO (12 Layers, 36 Channels)</th> </tr> </thead> <tbody> <tr> <td>Acoustic</td> <td>1.4x <em>slower</em></td> <td>27x <em>slower</em></td> <td>80x <em>slower</em></td> </tr> <tr> <td>VTI Elastic</td> <td>1.4x <em>slower</em></td> <td>3x <em>slower</em></td> <td>1.3x <em>slower</em></td> </tr> </tbody> </table> <h3 id="2-if-we-allow-the-fno-to-use-a-coarser-25m-grid-it-can-catch-up">2. If we allow the FNO to use a coarser 25m grid, it can catch up.</h3> <table> <thead> <tr> <th>Physics</th> <th>Small FNO</th> <th>Medium FNO</th> <th>Large FNO</th> </tr> </thead> <tbody> <tr> <td>Acoustic</td> <td>1.25x <em>faster</em></td> <td>2.7x <em>slower</em></td> <td>6x <em>slower</em></td> </tr> <tr> <td>VTI Elastic</td> <td>9.65x <em>faster</em></td> <td>2.89x <em>faster</em></td> <td>1.30x <em>faster</em></td> </tr> </tbody> </table> <p>Only the smallest networks (4 layers, 12 channels) achieve consistent speedups. Bigger models, even on coarse grids, are still slower.</p> <image></image> <h3 id="3-the-real-win-predicting-only-the-surface-wavefield">3. The real win? Predicting <em>only the surface</em> wavefield.</h3> <p>If you restrict the FNO to predicting the full time history at a single depth (like the surface), it avoids recurrent inference and runs extremely fast.</p> <table> <thead> <tr> <th>Model</th> <th>Speedup over FD</th> </tr> </thead> <tbody> <tr> <td>FNO-Small</td> <td>4200x</td> </tr> <tr> <td>FNO-Medium</td> <td>1260x</td> </tr> <tr> <td>FNO-Large</td> <td>568x</td> </tr> </tbody> </table> <image></image> <h2 id="why-are-fnos-slower">Why Are FNOs Slower?</h2> <p>The math is clear:</p> <ul> <li>FD scales as $O(p^d \beta^d N)$</li> <li>FNO scales as $O(L N \log N)$</li> </ul> <p>Where:</p> <ul> <li>$p$ = stencil width</li> <li>$\beta$ = oversampling factor</li> <li>$N$ = number of points</li> <li>$L$ = number of FNO layers</li> </ul> <p>In 3D, $p^d \beta^d$ grows fast, but not fast enough to offset the heavy lifting of FFTs unless the FNO network is <em>very</em> shallow.</p> <p>Also, FNOs carry a lot of constant-factor overhead: memory pressure, activation storage, and FFT setup costs.</p> <h2 id="when-are-fnos-useful">When Are FNOs Useful?</h2> <p>They <em>are</em> valuable in some cases:</p> <ul> <li><strong>Receiver-only predictions</strong> (e.g., marine acquisition)</li> <li><strong>Low-resolution surrogate models</strong> where full fidelity isn’t needed</li> </ul> <p>But for full-volume, high-res, time-domain simulations? They’re not competitive yet.</p> <h2 id="takeaways">Takeaways</h2> <ul> <li>FNOs are not faster than FD solvers for full-volume simulations</li> <li>Even on coarser grids, FNOs must be small to offer speedups</li> <li>Major gains come only when predicting surface fields (i.e., output dimensionality is reduced)</li> </ul> <p>If you’re designing an FNO for wave simulations, benchmark it properly against a tuned FD code. And if you’re aiming for performance, remember that “learned” doesn’t always mean “faster.”</p> <image></image> <h2 id="read-more">Read More</h2> <ul> <li><a href="https://www.devitoproject.org/">Devito finite difference framework</a></li> <li><a href="https://arxiv.org/abs/2010.08895">Fourier Neural Operator original paper</a></li> <li><a href="https://arxiv.org/abs/2310.00120">Tucker-Tensorized FNO</a></li> </ul> <hr/>]]></content><author><name>Dimitri Voytan</name></author><category term="NeuralOperators"/><category term="Ai4Science"/><summary type="html"><![CDATA[We challenge the performance benefits of Fourier Neural Operators, particularly for linear wave propagation problems]]></summary></entry></feed>